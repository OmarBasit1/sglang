#!/usr/bin/env python3
"""
Script to parse SGLang profiling logs and extract performance metrics.

This script processes log files generated by the profile_for_distserve.sh script
and creates a DataFrame with the following columns:
- model: Model name extracted from directory name
- batch_size: Batch size used in the benchmark
- input_size: Input sequence length
- tp_degree: Tensor parallelism degree (inferred from script structure)
- prefill_time: Prefill latency in seconds
- decode_time: Average decode latency in seconds (excluding Decode 0)

Usage:
    # Parse logs from current directory
    python parse_profile_logs.py .
    
    # Parse logs and save to specific file
    python parse_profile_logs.py /path/to/logs -o results.csv
    
    # Override TP degree for all entries
    python parse_profile_logs.py . --tp-degree 4
    
    # Parse multiple model directories
    python parse_profile_logs.py /path/to/all/profile/dirs

Example directory structure:
    base_directory/
    ├── Distserve_profile_Llama-3.1-8B-Instruct/
    │   ├── profile_input_4_output_16.log
    │   ├── profile_input_16_output_16.log
    │   └── ...
    ├── Distserve_profile_Qwen2.5-14B-Instruct/
    │   ├── profile_input_4_output_16.log
    │   └── ...
    └── parsed_profile_results.csv  # Generated output

For programmatic usage:
    from parse_profile_logs import parse_all_logs
    df = parse_all_logs('/path/to/logs')
"""

import os
import re
import pandas as pd
import argparse
from pathlib import Path
from typing import List, Dict, Tuple
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def extract_model_name(dir_path: str) -> str:
    """Extract model name from directory path."""
    dir_name = os.path.basename(dir_path)
    # Remove the "Distserve_profile_" prefix
    if dir_name.startswith("Distserve_profile_"):
        return dir_name[len("Distserve_profile_"):]
    return dir_name


def extract_input_size_and_tp(filename: str) -> Tuple[int, int]:
    """Extract input size and TP degree from filename."""
    match = re.search(r'profile_input_(\d+)_output_\d+_TP_(\d+).log', filename)
    if match:
        input_size = int(match.group(1))
        tp_degree = int(match.group(2))
        return input_size, tp_degree
    raise ValueError(f"Could not extract input size from filename: {filename}")


def parse_log_file(file_path: str, model_name: str, input_size: int, tp_degree: int = 1) -> List[Dict]:
    """Parse a single log file and extract benchmark data."""
    results = []
    
    try:
        with open(file_path, 'r') as f:
            content = f.read()
    except Exception as e:
        logger.error(f"Error reading file {file_path}: {e}")
        return results
    
    # Split into sections by looking for "Benchmark ..." followed by benchmark data
    # We want to skip the warmup section and only process benchmark sections
    
    # Find all benchmark sections (after "Benchmark ...")
    benchmark_sections = []
    lines = content.split('\n')
    
    current_section = []
    in_benchmark = False
    
    for line in lines:
        if line.strip() == "Benchmark ...":
            if current_section and in_benchmark:
                benchmark_sections.append('\n'.join(current_section))
            current_section = []
            in_benchmark = True
        elif in_benchmark:
            current_section.append(line)
    
    # Add the last section if it exists
    if current_section and in_benchmark:
        benchmark_sections.append('\n'.join(current_section))
    
    # Process each benchmark section
    for section in benchmark_sections:
        section_results = parse_benchmark_section(section, model_name, input_size, tp_degree)
        if section_results:
            results.extend(section_results)
    
    return results


def parse_benchmark_section(section: str, model_name: str, input_size: int, tp_degree: int) -> List[Dict]:
    """Parse a single benchmark section and extract metrics for all batch sizes."""
    lines = section.strip().split('\n')
    results = []
    
    current_prefill_time = None
    current_batch_size = None
    current_decode_times = []
    
    for line in lines:
        line = line.strip()
        
        # Extract prefill time
        prefill_match = re.search(r'Prefill\.\s+latency:\s+([\d.]+)\s+s', line)
        if prefill_match:
            # If we have accumulated data for previous batch size, save it
            if current_prefill_time is not None and current_batch_size is not None and current_decode_times:
                avg_decode_time = sum(current_decode_times) / len(current_decode_times)
                results.append({
                    'model': model_name,
                    'batch_size': current_batch_size,
                    'input_size': input_size,
                    'tp_degree': tp_degree,
                    'prefill_time': current_prefill_time,
                    'decode_time': avg_decode_time
                })
            
            # Start new batch size group
            current_prefill_time = float(prefill_match.group(1))
            current_batch_size = None
            current_decode_times = []
            continue
        
        # Extract decode times (skip Decode 0)
        decode_match = re.search(r'Decode\s+(\d+)\.\s+Batch size:\s+(\d+),\s+latency:\s+([\d.]+)\s+s', line)
        if decode_match:
            decode_step = int(decode_match.group(1))
            batch_size = int(decode_match.group(2))
            decode_latency = float(decode_match.group(3))
            
            # Set batch size from first decode entry
            if current_batch_size is None:
                current_batch_size = batch_size
            elif current_batch_size != batch_size:
                logger.warning(f"Batch size mismatch in section: expected {current_batch_size}, got {batch_size}")
            
            # Skip Decode < 2
            if decode_step > 2:
                current_decode_times.append(decode_latency)
    
    # Don't forget the last batch size group
    if current_prefill_time is not None and current_batch_size is not None and current_decode_times:
        avg_decode_time = sum(current_decode_times) / len(current_decode_times)
        results.append({
            'model': model_name,
            'batch_size': current_batch_size,
            'input_size': input_size,
            'tp_degree': tp_degree,
            'prefill_time': current_prefill_time,
            'decode_time': avg_decode_time
        })
    
    return results


def find_profile_directories(base_path: str) -> List[Tuple[str, str]]:
    """Find all profile directories and return (dir_path, model_name) tuples."""
    base_path = Path(base_path)
    profile_dirs = []
    
    for item in base_path.iterdir():
        if item.is_dir() and item.name.startswith("Distserve_profile_"):
            model_name = extract_model_name(str(item))
            profile_dirs.append((str(item), model_name))
    
    return profile_dirs


def parse_all_logs(base_directory: str) -> pd.DataFrame:
    """Parse all log files in the base directory and return a DataFrame."""
    all_results = []
    
    # Find all profile directories
    profile_dirs = find_profile_directories(base_directory)
    
    if not profile_dirs:
        logger.warning(f"No profile directories found in {base_directory}")
        return pd.DataFrame()
    
    for dir_path, model_name in profile_dirs:
        logger.info(f"Processing model: {model_name}")
        
        # Find all log files in this directory
        dir_path_obj = Path(dir_path)
        log_files = list(dir_path_obj.glob("profile_input_*_output_*.log"))
        
        for log_file in log_files:
            try:
                input_size, tp_degree = extract_input_size_and_tp(log_file.name)

                logger.info(f"  Processing file: {log_file.name} (input_size: {input_size}, tp_degree: {tp_degree})")
                
                results = parse_log_file(str(log_file), model_name, input_size, tp_degree)
                all_results.extend(results)
                
            except Exception as e:
                logger.error(f"Error processing file {log_file}: {e}")
                continue
    
    if not all_results:
        logger.warning("No results extracted from any log files")
        return pd.DataFrame()
    
    # Create DataFrame
    df = pd.DataFrame(all_results)
    
    # Sort by model, tp_degree, input_size, batch_size for better readability
    df = df.sort_values(['model', 'tp_degree', 'input_size', 'batch_size']).reset_index(drop=True)
    
    logger.info(f"Successfully parsed {len(df)} benchmark results")
    return df


def detect_tp_degree(log_file_path: str) -> int:
    """Try to detect TP degree from log file content."""
    try:
        with open(log_file_path, 'r') as f:
            content = f.read()
        
        # Look for TP indicators in the content
        # This might be extended based on actual log patterns when TP > 1 data is available
        tp_patterns = [
            r'TP(\d+)',
            r'tp-size[:\s]+(\d+)',
            r'tensor[_\s-]parallel[_\s-]size[:\s]+(\d+)',
        ]
        
        for pattern in tp_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        # If no TP indicators found, assume TP=1
        return 1
        
    except Exception as e:
        logger.warning(f"Could not detect TP degree from {log_file_path}: {e}")
        return 1


def main():
    parser = argparse.ArgumentParser(description="Parse SGLang profiling logs into a DataFrame")
    parser.add_argument("base_directory", help="Base directory containing profile directories")
    parser.add_argument("-o", "--output", help="Output CSV file path (optional)")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.base_directory):
        logger.error(f"Base directory does not exist: {args.base_directory}")
        return 1
    
    
    # Parse all logs
    df = parse_all_logs(args.base_directory)
    
    if df.empty:
        logger.error("No data was extracted from the log files")
        return 1
    
    # Print summary
    print(f"\nExtracted {len(df)} benchmark results:")
    print(f"Models: {df['model'].unique().tolist()}")
    print(f"Input sizes: {sorted(df['input_size'].unique().tolist())}")
    print(f"Batch sizes: {sorted(df['batch_size'].unique().tolist())}")
    print(f"TP degrees: {sorted(df['tp_degree'].unique().tolist())}")
    
    # Save to file if specified
    if args.output:
        df.to_csv(args.output, index=False)
        logger.info(f"Results saved to {args.output}")
    else:
        # Save to default location
        output_file = os.path.join(args.base_directory, "parsed_profile_results.csv")
        df.to_csv(output_file, index=False)
        logger.info(f"Results saved to {output_file}")
    
    return 0


def load_profile_data(base_directory: str, output_file: str = None, tp_degree_override: int = None) -> pd.DataFrame:
    """
    Convenience function for programmatic usage.
    
    Args:
        base_directory: Path to directory containing profile folders
        output_file: Optional CSV output file path
        tp_degree_override: Optional TP degree to override all detected values
    
    Returns:
        DataFrame with parsed profile data
    """
    df = parse_all_logs(base_directory, auto_detect_tp=True)
    
    if tp_degree_override is not None:
        df['tp_degree'] = tp_degree_override
    
    if output_file:
        df.to_csv(output_file, index=False)
        logger.info(f"Results saved to {output_file}")
    
    return df


if __name__ == "__main__":
    exit(main())
