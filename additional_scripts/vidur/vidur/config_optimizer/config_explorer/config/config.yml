clusters:
  - device: a100
    num_gpus: 8
    gpus_per_node: 8

schedulers:
  - scheduler: sarathi
    chunk_size: 1024
  # - scheduler: sarathi
  #   chunk_size: 2048


traces:
  - name: chat
    trace_file: "./data/processed_traces/sharegpt_rps0.5_processed.csv"
    max_seq_len: 4096
    num_requests: 80
    start_qps: 0.5

# batch_sizes: [32, 64, 128]
batch_sizes: [32]
tp_dimensions: [1, 2, 4, 8]
pp_dimensions: [1]

models:
  # - name: phi-2
  #   identifier: microsoft/phi-2
  #   exclude_tp_dims: [2, 4, 8]
  - name: opt-66b
    identifier: facebook/opt-66b
    exclude_tp_dims: [1, 2, 4]
  # - name: internlm-20b
  #   identifier: internlm/internlm-20b
  #   exclude_tp_dims: [1]
  # - name: codellama-34b-instruct-hf
  #   identifier: codellama/CodeLlama-34b-Instruct-hf
  # - name: llama-2-70b-hf
  #   identifier: meta-llama/Llama-2-70b-hf
  #   exclude_tp_dims: [1, 2, 4]
  # - name: qwen-72b
  #   identifier: Qwen/Qwen-72B
  #   exclude_tp_dims: [1]
  # - name: qwen-72b
  #   identifier: Qwen/Qwen-72B
  #   exclude_tp_dims: [1]
